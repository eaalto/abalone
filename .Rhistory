# The second, the data vector, isn't strictly necessary, but it's cleaner to pass it in rather than
# use the external variable
calcLikBH <- function(paramV, dataV) {
a       = paramV[1]    # finite growth rate
b       = paramV[2]    # density dependent parameter
sigma   = paramV[3]    # normal error
n0      = paramV[4]    # initial condition
numData = length(dataV)
NV      = numeric(numData) # number of observations
NV[1]   = n0         # set the initial condition
for (t in 1:(numData-1)) { NV[t+1] = a*NV[t] / (1+b*NV[t]) }  # simulate the system
# the true likelihood is computed as the product of the probability to observe the data we have, namely
#    L1=prod(dnorm(N,mean=Nobs,sd=sigma))
# where N and Nobs are the vectors of simulated abundances and observed data respectively
# and "prod" is the R function to compute the product of these probabilities
# We want to maximize L1 but, in practice, it is easier to log transform the product in a series of sums,
# put a negative sign in front of it and minimize it (as this is the way the R algorithm works)
# In practice:
L = -sum(dnorm(NV, mean=dataV, sd=sigma, log=TRUE))
return(L)  # return the log-likelihood value
}
# now let's use the R "optim" function to find the minimum of the LikBH function. We need to specify where we start from
# (i.e. provide an initial guess of model parameters)
# 'dataV' here is passed in to the function as well so its source is clear
maxLogBH = optim(par=c(2,0.02,20,100), fn=calcLikBH, dataV=obsNV); maxLogBH
print(maxLogBH$par)
# this function takes "params" as input and return the simulated abundances
# I.e. it generates a non-noisy population trajectory given our estimates
fitBH <- function(paramV, numData) {
a     = paramV[1]
b     = paramV[2]
sigma = paramV[3]
n0    = paramV[4]
NV    = numeric(numData)
NV[1] = n0
for (t in 1:(numData-1)) { NV[t+1] = a*NV[t] / (1+b*NV[t]) }
return(NV)
}
# The data
par(mfrow=c(1,1))
plot(trueNV) # true values
points(obsNV, col="blue") # observations
# The estimate from linear parameter regression
lines(fitBH(c(fitAlpha, fitBeta, 0, 100), length(obsNV)), col="green")
# The likelihood estimate
lines(fitBH(maxLogBH$par, length(obsNV)),col='red')
source('D:/Research/Code/class/maxLikelihood_examples.r')
source('D:/Research/Code/class/maxLikelihood_examples.r')
source('D:/Research/Code/class/maxLikelihood_examples.r')
source('D:/Research/Code/class/maxLikelihood_examples.r')
source('D:/Research/Code/class/maxLikelihood_examples.r')
source('D:/Research/Code/class/maxLikelihood_examples.r')
dev.off()
rm(list=ls(all=TRUE)) # removes all previous material from R's memory
MAX_T = 30  # time horizon
# Good practice to pre-allocate memory for large datasets
# Not so important here, but with 100K entries it might save a lot of processing time
trueNV    = numeric(MAX_T) # the true N values
obsNV     = numeric(MAX_T) # our observed N values
# Let's create a basic Beverton-Holt model
# Parameters
TRUE_K  = 1000    # carrying capacity
TRUE_A  = 1.4     # finite growth rate at low densities
TRUE_B  = (TRUE_A-1)/TRUE_K # density dependence term
TRUE_N0 = 100     # initial conditions
trueNV[1] = TRUE_N0
# and simulate the system
for (t in 1:(MAX_T-1)) {
trueNV[t+1] = TRUE_A*trueNV[t] / (1+TRUE_B*trueNV[t])
}
plot(trueNV, type="l")
###############################################################################
# now, let's assume that the the stock assessment provides information about densities with a gaussian error
sigma = max(trueNV)/10 # variance based off of maximum value
# create a synthetic series of abundance with "noise"
obsNV = trueNV + rnorm(MAX_T, mean=0, sd=sigma)
# can't have negative fish, and we'll set =1 to avoid division by zero errors
obsNV[obsNV<0] = 1
# and plot them
points(obsNV, col="blue")
#  N_t / N_t+1 = (1 + b*N_t) / a
#  N_t / N_t+1 = 1/a + b/a * N_t  = a lines
nnV = obsNV[-MAX_T] / obsNV[-1] # shift the vectors
nnD = data.frame(nt=obsNV[-MAX_T], nOverN=nnV)
plot(obsNV[-MAX_T], nnV, ylab="N_t/N_t+1", xlab="N_t")
sigma = max(trueNV)/10 # variance based off of maximum value
# create a synthetic series of abundance with "noise"
obsNV = trueNV + rnorm(MAX_T, mean=0, sd=sigma)
# can't have negative fish, and we'll set =1 to avoid division by zero errors
obsNV[obsNV<0] = 1
# and plot them
points(obsNV, col="blue")
# Quick way to estimate params:
#  N_t+1 = a*N_t / (1 + b*N_t)
# Rearrange:
#  N_t / N_t+1 = (1 + b*N_t) / a
#  N_t / N_t+1 = 1/a + b/a * N_t  = a lines
nnV = obsNV[-MAX_T] / obsNV[-1] # shift the vectors
nnD = data.frame(nt=obsNV[-MAX_T], nOverN=nnV)
plot(obsNV[-MAX_T], nnV, ylab="N_t/N_t+1", xlab="N_t")
fitMod = lm(nOverN~nt, nnD)
abline(coef(fitMod))
obsNV = trueNV + rnorm(MAX_T, mean=0, sd=sigma)
# can't have negative fish, and we'll set =1 to avoid division by zero errors
obsNV[obsNV<0] = 1
# and plot them
points(obsNV, col="blue")
# Quick way to estimate params:
#  N_t+1 = a*N_t / (1 + b*N_t)
# Rearrange:
#  N_t / N_t+1 = (1 + b*N_t) / a
#  N_t / N_t+1 = 1/a + b/a * N_t  = a lines
nnV = obsNV[-MAX_T] / obsNV[-1] # shift the vectors
nnD = data.frame(nt=obsNV[-MAX_T], nOverN=nnV)
plot(obsNV[-MAX_T], nnV, ylab="N_t/N_t+1", xlab="N_t")
obsNV = trueNV + rnorm(MAX_T, mean=0, sd=sigma)
# can't have negative fish, and we'll set =1 to avoid division by zero errors
obsNV[obsNV<0] = 1
# and plot them
points(obsNV, col="blue")
# Quick way to estimate params:
#  N_t+1 = a*N_t / (1 + b*N_t)
# Rearrange:
#  N_t / N_t+1 = (1 + b*N_t) / a
#  N_t / N_t+1 = 1/a + b/a * N_t  = a lines
nnV = obsNV[-MAX_T] / obsNV[-1] # shift the vectors
nnD = data.frame(nt=obsNV[-MAX_T], nOverN=nnV)
plot(obsNV[-MAX_T], nnV, ylab="N_t/N_t+1", xlab="N_t")
fitMod = lm(nOverN~nt, nnD)
obsNV = trueNV + rnorm(MAX_T, mean=0, sd=sigma)
# can't have negative fish, and we'll set =1 to avoid division by zero errors
obsNV[obsNV<0] = 1
# and plot them
points(obsNV, col="blue")
# Quick way to estimate params:
#  N_t+1 = a*N_t / (1 + b*N_t)
# Rearrange:
#  N_t / N_t+1 = (1 + b*N_t) / a
#  N_t / N_t+1 = 1/a + b/a * N_t  = a lines
nnV = obsNV[-MAX_T] / obsNV[-1] # shift the vectors
nnD = data.frame(nt=obsNV[-MAX_T], nOverN=nnV)
plot(obsNV[-MAX_T], nnV, ylab="N_t/N_t+1", xlab="N_t")
obsNV = trueNV + rnorm(MAX_T, mean=0, sd=sigma)
# can't have negative fish, and we'll set =1 to avoid division by zero errors
obsNV[obsNV<0] = 1
# and plot them
points(obsNV, col="blue")
# Quick way to estimate params:
#  N_t+1 = a*N_t / (1 + b*N_t)
# Rearrange:
#  N_t / N_t+1 = (1 + b*N_t) / a
#  N_t / N_t+1 = 1/a + b/a * N_t  = a lines
nnV = obsNV[-MAX_T] / obsNV[-1] # shift the vectors
nnD = data.frame(nt=obsNV[-MAX_T], nOverN=nnV)
plot(obsNV[-MAX_T], nnV, ylab="N_t/N_t+1", xlab="N_t")
fitMod = lm(nOverN~nt, nnD)
abline(coef(fitMod))
fitAlpha = 1/coef(fitMod)[1]
fitBeta  = fitAlpha*coef(fitMod)[2]
print(paste("alpha:", fitAlpha))
print(paste("beta:", fitBeta))
###############################################################################
## now, let's try to back estimate the parameters by using the maximum likelihood method, i.e. finding the model parameters that maximizes the probability of seeing the data that we see
# The first argument here must be the parameter vector
# The second, the data vector, isn't strictly necessary, but it's cleaner to pass it in rather than
# use the external variable
calcLikBH <- function(paramV, dataV) {
a       = paramV[1]    # finite growth rate
b       = paramV[2]    # density dependent parameter
sigma   = paramV[3]    # normal error
n0      = paramV[4]    # initial condition
numData = length(dataV)
NV      = numeric(numData) # number of observations
NV[1]   = n0         # set the initial condition
for (t in 1:(numData-1)) { NV[t+1] = a*NV[t] / (1+b*NV[t]) }  # simulate the system
# the true likelihood is computed as the product of the probability to observe the data we have, namely
#    L1=prod(dnorm(N,mean=Nobs,sd=sigma))
# where N and Nobs are the vectors of simulated abundances and observed data respectively
# and "prod" is the R function to compute the product of these probabilities
# We want to maximize L1 but, in practice, it is easier to log transform the product in a series of sums,
# put a negative sign in front of it and minimize it (as this is the way the R algorithm works)
# In practice:
L = -sum(dnorm(NV, mean=dataV, sd=sigma, log=TRUE))
return(L)  # return the log-likelihood value
}
# now let's use the R "optim" function to find the minimum of the LikBH function. We need to specify where we start from
# (i.e. provide an initial guess of model parameters)
# 'dataV' here is passed in to the function as well so its source is clear
maxLogBH = optim(par=c(2,0.02,20,100), fn=calcLikBH, dataV=obsNV); maxLogBH
print(maxLogBH$par)
# this function takes "params" as input and return the simulated abundances
# I.e. it generates a non-noisy population trajectory given our estimates
fitBH <- function(paramV, numData) {
a     = paramV[1]
b     = paramV[2]
sigma = paramV[3]
n0    = paramV[4]
NV    = numeric(numData)
NV[1] = n0
for (t in 1:(numData-1)) { NV[t+1] = a*NV[t] / (1+b*NV[t]) }
return(NV)
}
# The data
par(mfrow=c(1,1))
plot(trueNV, type="l", ylim=c(0, max(c(obsNV, trueNV)))) # true values
points(obsNV, col="blue") # observations
# The estimate from linear parameter regression
lines(fitBH(c(fitAlpha, fitBeta, 0, 100), length(obsNV)), col="green")
# The likelihood estimate
lines(fitBH(maxLogBH$par, length(obsNV)),col='red')
# The true curve
#lines(trueNV, col='black')
# Many ways to estimate uncertainty
# One simple one is profile likelihood
# Make a list of alternate alpha and beta values
aValV = seq(1.3, 1.6, length=100)
bValV = seq(0.0003, 0.0006, length=100)
# convert from log-likelihood to likelihood
maxL = exp(-maxLogBH$value)
maxL
maxL
# convert from log-likelihood to likelihood
maxL = exp(-maxLogBH$value)
# Two functional wrappers which change only the parameter of interest
profileBHA <- function(x) { exp(-calcLikBH(c(x, maxLogBH$par[-1]), obsNV)) }
profileBHB <- function(x) { exp(-calcLikBH(c(maxLogBH$par[1], x, maxLogBH$par[c(3,4)]), obsNV)) }
# apply the functions to the vectors
aLV  = sapply(aValV, profileBHA) / maxL
bLV  = sapply(bValV, profileBHB) / maxL
# plot the parameter space and the L ratio
par(mfrow=c(1,2))
plot(aValV, aLV, xlab="alpha", ylab="L ratio", type="l")
# 95% interval is roughly equal to 0.174
abline(0.147, 0, col="red")
plot(bValV, bLV, xlab="beta", ylab="L ratio", type="l")
abline(0.147, 0, col="red")
#############################################
####  Model comparison
#############################################
# Let's compare two possible models for some data, and find which fits best
# BH     - N_t+1 = a*N_t / (1 + b*N_t)
# Ricker - N_t+1 = N_t * e^(a-aN_t/K)
if (runif(1)>0.5) {
print("Generating Beverton-Holt..")
if (runif(1)>0.5) {
print("Generating Beverton-Holt..")
if (runif(1)>0.5) {
print("Generating Beverton-Holt..")
# generate BH data
for (t in 1:(MAX_T-1)) trueNV[t+1] = TRUE_A*trueNV[t] / (1+TRUE_B*trueNV[t])
} else {
print("Generating Ricker...")
# generate Ricker data
for (t in 1:(MAX_T-1)) trueNV[t+1] = trueNV[t] * exp(log(TRUE_A)*(1-trueNV[t]/TRUE_K));
}
sigma = max(trueNV)/10 # variance based off of maximum value
obsNV = trueNV + rnorm(MAX_T, mean=0, sd=sigma)
obsNV[obsNV<0] = 0
rickerFn <- function(a, K, N0, maxT) {
NV    = numeric(maxT)
NV[1] = N0
for (t in 1:(maxT-1)) { NV[t+1] = a*NV[t] * exp(-NV[t]/K) }  # simulate the system
return(NV)
}
calcLikRicker <- function(paramV, dataV) {
a     = paramV[1]     # finite growth rate
K     = paramV[2]     # carrying capacity
sigma = paramV[3]     # normal error
n0    = paramV[4]     # initial condition
maxT  = length(dataV) # number of observations
NV = rickerFn(a, K, n0, maxT)
calcLikRicker <- function(paramV, dataV) {
a     = paramV[1]     # finite growth rate
K     = paramV[2]     # carrying capacity
sigma = paramV[3]     # normal error
n0    = paramV[4]     # initial condition
maxT  = length(dataV) # number of observations
NV = rickerFn(a, K, n0, maxT)
L = -sum(dnorm(NV, mean=dataV, sd=sigma, log=TRUE))
return(L)  # return the log-likelihood value
}
# And the Ricker fitting
# In fact, looks like these are just special cases of one function
# We'll clean that up next time
fitRicker <- function(paramV, maxT) {
a     = paramV[1]
K     = paramV[2]
sigma = paramV[3]  # ignore this
n0    = paramV[4]
NV = rickerFn(a, K, n0, maxT)
return(NV)
}
# let's make our display easier to read with a shorthand formating function
myF <- function(value) { format(value, digits=3) }
maxLogBH = optim(par=c(2,0.02,20,100), fn=calcLikBH, dataV=obsNV);
print(paste("BH Log-lik:", -maxLogBH$value))
namesV = c("alpha", "beta", "sigma", "N0")
parV   = maxLogBH$par
# use the formatting here
print(list(namesV, myF(parV)))
# Add some additional iteration space to help it converge.  Default is 500
maxLogRicker = optim(par=c(2,500,20,100), fn=calcLikRicker, dataV=obsNV, control=list(maxit=1000));
print(paste("Ricker Log-lik:", -maxLogRicker$value))
namesV[2] = "K"
parV = maxLogRicker$par
print(list(namesV, myF(parV)))
# restore the normal arrangement
par(mfrow=c(1,1))
# The data
plot(obsNV, col="blue", xlab="Time", ylab="Abundance")
# The BH estimate
lines(fitBH(maxLogBH$par, length(obsNV)), col='red')
# The Ricker estimate
lines(fitRicker(maxLogRicker$par, length(obsNV)), col='green')
maxLogRicker = optim(par=c(2,500,20,100), fn=calcLikRicker, dataV=obsNV, control=list(maxit=1000));
print(paste("Ricker Log-lik:", -maxLogRicker$value))
namesV[2] = "K"
parV = maxLogRicker$par
print(list(namesV, myF(parV)))
# restore the normal arrangement
par(mfrow=c(1,1))
# The data
plot(obsNV, col="blue", xlab="Time", ylab="Abundance")
# The BH estimate
lines(fitBH(maxLogBH$par, length(obsNV)), col='red')
# The Ricker estimate
lines(fitRicker(maxLogRicker$par, length(obsNV)), col='green')
maxLogRicker = optim(par=c(1.5,500,20,100), fn=calcLikRicker, dataV=obsNV, control=list(maxit=1000));
print(paste("Ricker Log-lik:", -maxLogRicker$value))
namesV[2] = "K"
parV = maxLogRicker$par
print(list(namesV, myF(parV)))
# restore the normal arrangement
par(mfrow=c(1,1))
# The data
plot(obsNV, col="blue", xlab="Time", ylab="Abundance")
# The BH estimate
lines(fitBH(maxLogBH$par, length(obsNV)), col='red')
# The Ricker estimate
lines(fitRicker(maxLogRicker$par, length(obsNV)), col='green')
envV = sin(1:MAX_T)
plot(envV, xlab="Time", ylab="Driver")
lines(envV)
# Maximum effect of the driver
TRUE_D = 0.2  # true value is 20%
# let's make a separate time series
true_envNV = numeric(MAX_T)
true_envNV[1] = 100
# generate the time series
# For real code, I'd want to make a flexible function which generates time series based on
# a variety of parameters so as not to cut-and-paste code all over
# We'll keep it simple here
for (t in 1:(MAX_T-1)) {
growVal     = TRUE_A*(1+TRUE_D*envV[t])
true_envNV[t+1] = growVal*true_envNV[t] / (1+TRUE_B*true_envNV[t])
}
# Add observation error again
sigma     = max(true_envNV)/10
obs_envNV = true_envNV + rnorm(MAX_T, mean=0, sd=sigma)
obs_envNV[obs_envNV<0] = 0
plot(true_envNV, type="l")
points(obs_envNV, col="blue")
# New likelihood model for the environmental driver
# Two convenience functions
# Notice that 'fit' now takes the data, not just the length
calcLikBHEnv <- function(paramV, dataV, envV) { bHEnvModFn(paramV, dataV, envV, getLik=TRUE) }
fitBHEnv <- function(paramV, dataV, envV) { bHEnvModFn(paramV, dataV, envV, getLik=FALSE) }
# One generic
bHEnvModFn <- function(paramV, dataV, envV, getLik=TRUE) {
a       = paramV[1]    # finite growth rate
b       = paramV[2]    # density dependent parameter
sigma   = paramV[3]    # normal error
n0      = paramV[4]    # initial condition
delta   = paramV[5]    # env driver
numData = length(dataV)
NV      = numeric(numData) # number of observations
NV[1]   = n0         # set the initial condition
for (t in 1:(numData-1)) { NV[t+1] = a*(1+delta*envV[t])*NV[t] / (1+b*NV[t]) }  # simulate the system
if (getLik) {
# return likelihood
L = -sum(dnorm(NV, mean=dataV, sd=sigma, log=TRUE))
# As parameter space grows, the optimizer sometimes heads off to strange areas involving negative params
# It helps to disallow that with an extreme penalty
if (sum(paramV<0)>0) L = L + 999*(-sum(paramV[paramV<0]))
return(L)
} else return(NV) # return the fit time series
}
# version with no driver
maxLogBH = optim(par=c(2,0.02,20,100), fn=calcLikBH, dataV=obs_envNV)
print(paste("BH Log-lik:", -maxLogBH$value))
namesV = c("alpha", "beta", "sigma", "N0")
parV   = maxLogBH$par
# use the formatting here
print(list(namesV, myF(parV)))
# version with environmental driver
maxLogBHEnv = optim(par=c(2,0.02,20,100, 0.1), fn=calcLikBHEnv, dataV=obs_envNV, envV=envV, control=list(maxit=1000))
print(paste("BH-Env Log-lik:", -maxLogBHEnv$value))
namesV = c("alpha", "beta", "sigma", "N0", "delta")
parV   = maxLogBHEnv$par
# use the formatting here
print(list(namesV, myF(parV)))
# The BH estimate
# Still using old function
lines(fitBH(maxLogBH$par, length(obs_envNV)), col='red')
# The BH-Env estimate
lines(fitBHEnv(maxLogBHEnv$par, obs_envNV, envV), col='green')
effortV    = numeric(MAX_T)
effortV[1] = 2
for (t in 1:(MAX_T-1)) { effortV[t+1] = 1.5*effortV[t] / (1+(0.5/100)*effortV[t]) }
plot(effortV, type="l", xlab="Time", ylab="Boats")
# Now the true abundance and catch
TRUE_Q  = 0.0025  # each boat can catch 0.25% of the stock
SIGMA_Q = 0.0005 # SD around that catch
catchV    = numeric(MAX_T)
trueCatNV = numeric(MAX_T)
trueCatNV[1] = TRUE_N0
# let catch efficieny vary a little
randCV  = rnorm(MAX_T, mean=TRUE_Q, sd=SIGMA_Q)
plot(randCV)
randCV[randCV<0] = 0
for (t in 1:(MAX_T-1)) {
# dynamics before catch
trueCatNV[t+1] = TRUE_A*trueCatNV[t] / (1+TRUE_B*trueCatNV[t])
# assume catch is *after* reproduction
catchMort   = exp(-randCV[t]*effortV[t])
catchV[t]   = trueCatNV[t+1] * (1-catchMort)
trueCatNV[t+1] = trueCatNV[t+1] * catchMort
}
# add last year catch
catchV[MAX_T]   = TRUE_A*trueCatNV[MAX_T] / (1+TRUE_B*trueCatNV[MAX_T]) * (1-exp(-randCV[MAX_T]*effortV[MAX_T]))
plot(trueCatNV, type="l", ylim=c(0, max(trueCatNV)))
points(catchV, col="red")
# Now we know effort and catch, and must try to estimate abundance
calcLikBHCatch <- function(paramV, catchV, effortV) { bHCatchModFn(paramV, catchV, effortV, getLik=TRUE) }
fitBHCatch <- function(paramV, catchV, effortV) { bHCatchModFn(paramV, catchV, effortV, getLik=FALSE) }
bHCatchModFn <- function(paramV, catchV, effortV, getLik=TRUE) {
# assume we know max growth rate
# difficult to resolve optimization otherwise
a = TRUE_A
b       = paramV[1]    # density dependence parameter
q       = paramV[2]    # catch efficiency
n0      = paramV[3]    # initial condition
#	a     = paramV[4]
maxT    = length(catchV)
NV      = numeric(maxT)
CV      = numeric(maxT) # number of catch records
NV[1]   = n0               # set the initial condition
for (t in 1:(maxT-1)) {
NV[t+1] = a*NV[t] / (1+b*NV[t])
# assume catch is *after* reproduction
cMort   = exp(-q*effortV[t])
CV[t]   = NV[t+1] * (1-cMort)
NV[t+1] = NV[t+1] * cMort
}
# last time step catch
CV[maxT] = a*NV[maxT] / (1+b*NV[maxT]) * (1-exp(-q*effortV[maxT]))
# LL based off of catch observations now
# Using true SIGMA_Q to aid convergence
if (getLik) {
# return likelihood
L = -sum(dnorm(CV, mean=catchV, sd=SIGMA_Q, log=TRUE))
if (sum(paramV<0)>0) L = L + 999*(-sum(paramV[paramV<0]))
return(L)
} else return(list(catch=CV, abun=NV)) # return both fit time series
}
maxLogBHCatch = optim(par=c(0.02,0.003,100), fn=calcLikBHCatch, catchV=catchV, effortV=effortV, control=list(maxit=1000))
print("BH Catch:")
namesV = c("beta", "q", "N0")
parV   = maxLogBHCatch$par
print(list(namesV, myF(parV)))
outputL = fitBHCatch(maxLogBHCatch$par, catchV, effortV)
# The catch estimate
lines(outputL$catch, col='red')
# The abundance estimate
lines(outputL$abun, col='green')
source('D:/Research/Code/class/maxLikelihood_examples.r')
source('D:/Research/Code/class/maxLikelihood_examples.r')
source('D:/Research/Code/abalone/ipm_model.R')
min(rnorm(1000, 0.01, 0.003))
min(rnorm(1000, 0.01, 0.003))
min(rnorm(1000, 0.01, 0.003))
sum(rnorm(1000, 0.01, 0.003)<0)
sum(rnorm(1000, 0.01, 0.003)<0)
sum(rnorm(1000, 0.01, 0.003)<0)
sum(rnorm(1000, 0.01, 0.003)<0)
sum(rnorm(1000, 0.01, 0.003)<0)
sum(rnorm(1000, 0.01, 0.003)<0)
sum(rnorm(100000, 0.01, 0.003)<0)
sum(rnorm(100000, 0.01, 0.003)<0)
sum(rnorm(100000, 0.01, 0.003)<0)
mB <- function(p1,p2,m) { p1%%m>p2}
mB
mB(4,2,1)
4%%1
mB(4,2,2)
mB(4,2,3)
mB(7,2,4)
testV = rnorm(10)
set.seed(123)
testV = rnorm(10)
set.seed(123)
testV2 = rnorm(10, mean=1)
testV
testV2
plot(testV)
plot(testV2)
source('D:/Research/Code/common/myUtil.r')
xmasCalc
xmasCalc()
setwd("D:/Research/Code/abalone")
source('D:/Research/Code/abalone/ipm_model.R')
runContinuum(paramStr=PARAM_DISC)
